#!/usr/bin/env python3

'''
Talk2Biomodels: A Streamlit app for the Talk2Biomodels graph.
'''

import os
import sys
import random
import streamlit as st
import pandas as pd
from streamlit_feedback import streamlit_feedback
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tracers.context import collect_runs
from langchain.callbacks.tracers import LangChainTracer
from utils import streamlit_utils
sys.path.append('./')
from aiagents4pharma.talk2biomodels.agents.t2b_agent import get_app
# from talk2biomodels.agents.t2b_agent import get_app

st.set_page_config(page_title="Talk2Biomodels", page_icon="ü§ñ", layout="wide")


# st.logo(
#     image='docs/VPE.png',
#     size='large',
#     link='https://github.com/VirtualPatientEngine'
# )

# Check if env variable OPENAI_API_KEY exists
if "OPENAI_API_KEY" not in os.environ:
    st.error("Please set the OPENAI_API_KEY environment \
        variable in the terminal where you run the app.")
    st.stop()

# Create a chat prompt template
prompt = ChatPromptTemplate.from_messages([
        ("system", "Welcome to Talk2Biomodels!"),
        MessagesPlaceholder(variable_name='chat_history', optional=True),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
])

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Initialize sbml_file_path
if "sbml_file_path" not in st.session_state:
    st.session_state.sbml_file_path = None

# Initialize project_name for Langsmith
if "project_name" not in st.session_state:
    # st.session_state.project_name = str(st.session_state.user_name) + '@' + str(uuid.uuid4())
    st.session_state.project_name = 'T2B-' + str(random.randint(1000, 9999))

# Initialize run_id for Langsmith
if "run_id" not in st.session_state:
    st.session_state.run_id = None

# Initialize graph
if "unique_id" not in st.session_state:
    st.session_state.unique_id = random.randint(1, 1000)
if "app" not in st.session_state:
    if "llm_model" not in st.session_state:
        st.session_state.app = get_app(st.session_state.unique_id)
    else:
        st.session_state.app = get_app(st.session_state.unique_id,
                                       llm_model=st.session_state.llm_model)

# Get the app
app = st.session_state.app

# Main layout of the app split into two columns
main_col1, main_col2 = st.columns([3, 7])
# First column
with main_col1:
    with st.container(border=True):
        # Title
        st.write("""
            <h3 style='margin: 0px; padding-bottom: 10px; font-weight: bold;'>
            ü§ñ Talk2Biomodels
            </h3>
            """,
            unsafe_allow_html=True)

        # LLM panel (Only at the front-end for now)
        llms = ["gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]
        st.selectbox(
            "Pick an LLM to power the agent",
            llms,
            index=0,
            key="llm_model",
            on_change=streamlit_utils.update_llm_model
        )

        # Upload files (placeholder)
        uploaded_file = st.file_uploader(
            "Upload an XML/SBML file",
            accept_multiple_files=False,
            type=["xml", "sbml"],
            help='''Upload an XML/SBML file to simulate
                a biological model, and ask questions
                about the simulation results.'''
            )

        # Help text
        st.button("Know more ‚Üó",
                #   icon="‚ÑπÔ∏è",
                  on_click=streamlit_utils.help_button,
                  use_container_width=False)

    with st.container(border=False, height=500):
        prompt = st.chat_input("Say something ...", key="st_chat_input")

# Second column
with main_col2:
    # Chat history panel
    with st.container(border=True, height=575):
        st.write("#### üí¨ Chat History")

        # Display chat messages
        for count, message in enumerate(st.session_state.messages):
            if message["type"] == "message":
                with st.chat_message(message["content"].role,
                                     avatar="ü§ñ"
                                     if message["content"].role != 'user'
                                     else "üë©üèª‚Äçüíª"):
                    st.markdown(message["content"].content)
                    st.empty()
            elif message["type"] == "plotly":
                streamlit_utils.render_plotly(message["content"],
                              key=message["key"],
                              tool_name=message["tool_name"],
                              save_chart=False)
                st.empty()
            elif message["type"] == "toggle":
                streamlit_utils.render_toggle(key=message["key"],
                                    toggle_text=message["content"],
                                    toggle_state=message["toggle_state"],
                                    save_toggle=False)
                st.empty()
            elif message["type"] == "dataframe":
                streamlit_utils.render_table(message["content"],
                                key=message["key"],
                                tool_name=message["tool_name"],
                                save_table=False)
                st.empty()

        # When the user asks a question
        if prompt:
            # Create a key 'uploaded_file' to read the uploaded file
            if uploaded_file:
                st.session_state.sbml_file_path = uploaded_file.read().decode("utf-8")

            # Display user prompt
            prompt_msg = ChatMessage(prompt, role="user")
            st.session_state.messages.append(
                {
                    "type": "message",
                    "content": prompt_msg
                }
            )
            with st.chat_message("user", avatar="üë©üèª‚Äçüíª"):
                st.markdown(prompt)
                st.empty()

            with st.chat_message("assistant", avatar="ü§ñ"):
                # with st.spinner("Fetching response ..."):
                with st.spinner():
                    # Get chat history
                    history = [(m["content"].role, m["content"].content)
                                            for m in st.session_state.messages
                                            if m["type"] == "message"]
                    # Convert chat history to ChatMessage objects
                    chat_history = [
                        SystemMessage(content=m[1]) if m[0] == "system" else
                        HumanMessage(content=m[1]) if m[0] == "human" else
                        AIMessage(content=m[1])
                        for m in history
                    ]

                    # Create config for the agent
                    config = {"configurable": {"thread_id": st.session_state.unique_id}}
                    # Update the agent state with the selected LLM model
                    current_state = app.get_state(config)
                    app.update_state(
                        config,
                        {"sbml_file_path": [st.session_state.sbml_file_path]}
                    )
                    app.update_state(
                        config,
                        {"llm_model": st.session_state.llm_model}
                    )
                    # print (current_state.values)
                    # current_state = app.get_state(config)
                    # print ('updated state', current_state.values["sbml_file_path"])

                    ERROR_FLAG = False
                    with collect_runs() as cb:
                        # Add Langsmith tracer
                        tracer = LangChainTracer(project_name=st.session_state.project_name)
                        # Get response from the agent
                        response = app.invoke(
                            {"messages": [HumanMessage(content=prompt)]},
                            config=config|{"callbacks": [tracer]}
                        )
                        st.session_state.run_id = cb.traced_runs[-1].id
                    # print(response["messages"])
                    current_state = app.get_state(config)
                    # print (current_state.values["model_id"])

                    # Add response to chat history
                    assistant_msg = ChatMessage(
                                        response["messages"][-1].content,
                                        role="assistant")
                    st.session_state.messages.append({
                                    "type": "message",
                                    "content": assistant_msg
                                })
                    # Display the response in the chat
                    st.markdown(response["messages"][-1].content)
                    st.empty()
                    # Get the current state of the graph
                    current_state = app.get_state(config)
                    # Get the messages from the current state
                    # and reverse the order
                    reversed_messages = current_state.values["messages"][::-1]
                    # Loop through the reversed messages until a 
                    # HumanMessage is found i.e. the last message 
                    # from the user. This is to display the results
                    # of the tool calls made by the agent since the
                    # last message from the user.
                    for msg in reversed_messages:
                        # print (msg)
                        # Break the loop if the message is a HumanMessage
                        # i.e. the last message from the user
                        if isinstance(msg, HumanMessage):
                            break
                        # Skip the message if it is an AIMessage
                        # i.e. a message from the agent. An agent
                        # may make multiple tool calls before the
                        # final response to the user.
                        if isinstance(msg, AIMessage):
                            continue
                        # Work on the message if it is a ToolMessage
                        # These may contain additional visuals that
                        # need to be displayed to the user.
                        # print("ToolMessage", msg)
                        if msg.name in ["simulate_model", "custom_plotter"]:
                            df_simulated = pd.DataFrame.from_dict(
                                                current_state.values["dic_simulated_data"])
                            if msg.name == "simulate_model":
                                df_selected = df_simulated
                            else:
                                # Add Time column to the custom headers
                                custom_headers = msg.artifact
                                if custom_headers:
                                    if 'Time' not in msg.artifact:
                                        custom_headers = ['Time'] + custom_headers
                                    # Make df with only the custom headers
                                    df_selected = df_simulated[custom_headers]
                                else:
                                    continue
                            # Display the toggle button to suppress the table
                            streamlit_utils.render_toggle(
                                key="toggle_plotly_"+msg.name+'_'+str(st.session_state.run_id),
                                toggle_text="Show Plot",
                                toggle_state=True,
                                save_toggle=True)
                            # Display the plotly chart
                            streamlit_utils.render_plotly(
                                df_selected,
                                key="plotly_"+msg.name+'_'+str(st.session_state.run_id),
                                tool_name=msg.name,
                                save_chart=True)
                            # Display the toggle button to suppress the table
                            streamlit_utils.render_toggle(
                                key="toggle_table_"+msg.name+'_'+str(st.session_state.run_id),
                                toggle_text="Show Table",
                                toggle_state=False,
                                save_toggle=True)
                            # Display the table
                            streamlit_utils.render_table(
                                df_selected,
                                key="dataframe_"+msg.name+'_'+str(st.session_state.run_id),
                                tool_name=msg.name,
                                save_table=True)
                            st.empty()
                        elif msg.name in ["ask_question"]:
                            df_simulated = pd.DataFrame.from_dict(
                                                current_state.values["dic_simulated_data"])
                            # Display the toggle button to suppress the table
                            streamlit_utils.render_toggle(
                                key="toggle_table_"+msg.name+'_'+str(st.session_state.run_id),
                                toggle_text="Show Table",
                                toggle_state=False,
                                save_toggle=True)
                            # Display the table
                            streamlit_utils.render_table(
                                df_simulated,
                                key="dataframe_"+msg.name+'_'+str(st.session_state.run_id),
                                tool_name=msg.name,
                                save_table=True)
                            st.empty()
        # Collect feedback and display the thumbs feedback
        if st.session_state.get("run_id"):
            feedback = streamlit_feedback(
                feedback_type="thumbs",
                optional_text_label="[Optional] Please provide an explanation",
                on_submit=streamlit_utils.submit_feedback,
                key=f"feedback_{st.session_state.run_id}"
            )
